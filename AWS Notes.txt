S3 Service :- 

1. S3 is a object based storage.It allows u to upload files
2. S3 is not for block storage.U cannot install operaitng system or a database.
3. File size in S3 can be between 0 to 5 TB
4. S3 has unlimited storage.
5. Files are stored in buckets.Buckets are folders.
6. S3 buckets have universal namespace.Names of the buckets should be unique globally.
7. S3 bucket endpoint or URL convention is like - https://s3-<regionn>.amazonaws.com/<bucketname>
8. Read/Write consitancy
	8.1 Add Object - Read after write consistency for puts of new object.This means if u add a object and immediately try to access it.It ll be immediately availabe.
	8.2 Modify or Delete Object - Eventual consistency for overwrite puts and delete,It means once u modify or delete an object and then immediatey try to access it.It may or may not be reflected immediately.
9. Core fundamentals of S3.
	9.1 Key (name of the object)
	9.2 Value (Actual data)
	9.3 Version id
	9.4 Metadata (Data of data)
	9.5 Subresources
		9.5.1 Access control lists
		9.5.2 Torrent - Support for Bit torrent protocol
10. Versioning
	10.1 Stores all versions of an object(Including all writes and even if u delete an object)
	10.2 You pay for each version of the object
	10.3 Great backup tool
	10.4 Once enabled cannont be disabled it can only be suspended.
	10.5 Versioning integrates with life cycle rules
	10.6 Versioning provides multi factor authentication delete capability to provide an additional layer of security

11 Cross region replication
	11.1 Requires versioning to be enabled on source bucket as well as on destination bucket.
	11.2 Regions must be unique
	11.3 Files in existing buckets are not replicated automatically.All subsequent updated files will be replicated automatically.
	11.4 You cannot replicate to multiple buckets or use daisy chaining (at this time)
	11.5 Delete markers are replicated.
	11.6 Deleting individual versions or delete markers will not be replicated


12 Life cycle management 
	12.1 It can be used in conjuction wih versioning  or can be used independently
	12.2 Can be applied to all current versions and previous versions.Object should be minimum of 128kb.
	12.3 Following actions for eg can be done 
		12.3.1 Transition to standard infrequent access - 30 days after the date of creation.File size must be atleast 128KB in size
		12.3.2 Archive to glacier (30 days after moving into IA)
		12.3.3 You can also permanently delete objects using life cycle management rules

13. Edge Location - Location where content is cached.This is seperate to AWS Region/AZ.Edge locations are not only read only.You can write or put object on them.Objects are cached in edge locations for TTL(Time to live).TTL is always specified in seconds.By default objects are cached for 24 hours.U can clear the cache but u ll be charged for that. 

14. Origin - THis is the origin of all files that CDN ll distribute.It can be S3,EC2,ELB or Route53
15. Distribution - THis is the name given to the CDN which consists of collection of Edge Locations
16. Types of distribution
	16.1 Web distribution (Typically used for websites)
	16.2 RTMP (Used for media streaming)

17. By default all newly created buckets are private.
18. Access control on buckets 
	18.1 Bucket policies
	18.2 Access control list
19. S3 buckets can be configured to create access logs.The logs can be logged in to another bucket of same account or any other account.
20. When u write successfully to S3 , you get http 200 response code
21. Files can be loaded in S3 faster by enabling multipart upload.
22. When trying to grant an amazon account access to S3 using access control lists,The email address of the account or the canonical user ID should be used to identify the account.
23. Export to amazon glacier is not supported by AWS Import/Export
24. AWS Import/Export allows for the importation of large data sets, using external hard disks which are sent directly to amazon, therefore bypassing the internet.
25. You can retrive 10 GB of data from amazon glacier per month for free
26. Transfer Acceleration -
	* S3 Transfer Acceleration utilizes cloud front edge network to accelerate your uploads to S3.Instead of uploding directly to your S3 bucket,you can use a distinct URL to upload directly to an edge location which will then transfer that file to S3. 
	* Your data transfer application must use one of the following two types of endpoints to access the bucket for faster data transfer: <bucketname>.s3-accelerate.amazonaws.com or <bucketname>.s3-accelerate.dualstack.amazonaws.com for the “dual-stack” endpoint. If you want to use standard data transfer, you can continue to use the regular endpoints.
27. IPv6 support is not currently available when using Website Hosting and access via BitTorrent. All other features should work as expected when accessing Amazon S3 using IPv6.
28. Any publicly available data in Amazon S3 can be downloaded via the BitTorrent protocol, in addition to the default client/server delivery mechanism. Simply add the ?torrent parameter at the end of your GET request in the REST API.BitTorrent allows developers to further save on bandwidth costs for a popular piece of data by letting users download from Amazon and other users simultaneously

Note -  Read S3 FAQ  


* Storage Tiers/Classes
1. S3 (Durable,Immediately available , frequently accessed) Durability of 99.999999999%, Available 99.99%.Stored redundantly across  multiple devices in multiple facilities and is Designed to sustain loss of 2 facilities concurrently.
2. S3 - IA(Infrequent Access) (Durable,Immediately available , Infrequently accessed) Durability of 99.999999999%, Available 99.9%
3. Reduced Redundancy Storage (RRS) (Data that is easily reproduciable like thumbnails.) Durability of 99.99%
4. Glacier (Not immediately available.Takes up to 3 to 5 hours before accessing.Cheapest storage tier.It is used to archive data)


* Storage Gateways
1. File Gateway (Only used for flat files.Not applicable for block storage like OS or databases.Nothing is stored on premises.Everything is stored on S3)
2. Storage volumes (Used for blocked and flat file storage.The entire dataset is stored in local on premises data center and async to S3 & EBS.Eg For a finance analytics company they need to have lowest possibe latency hence every thing is available locally and also the data is critical hence backed up in AWS cloud)
3. Cached Volumes (Used for block and flat file storage.The entire data set is stored on AWS EBS and S3.The most frequntly accessed data is then stored on premises data center.)
4. Gateway Virtual Tape Library (VTL are mostly used for backups and used popular backup applications like Net Backup, Backup exec ,Veeam etc) 


* Encrytion Types
	1. In Transit	
		1.1 SSL/TLS (HTTPS)
	2. At rest
		2.1 Server Side Encryption 
			2.1.1 SSE - S3 (Managed encryption keys by AWS.256 bit encryption)
			2.1.2 SSE - KMS (Key Management Service.Paid service.Provides envlope keys for encryption keys.Provision for logging to check who and when accessed the keys)
			2.1.3 SSE - C (Customer managed encryption keys.AWS does not manage the encryption keys.)
	3. Client side encryption - Encryption of data at client side then uploading the data in S3


* Cloud Front
	1. Terminology	
		1.1 Egde Location - THis is the location where content ll be cached. This is separate to an AWS Region/AZ.
		1.2 Origin - THis is the origin of all the files that the CDN will distribute.This can be either S3 Bucket,an EC2 instance, Elastic Load Balancer or Route53 
		1.3 Distribution is the name given to the CDN which consists of a collection of Edge Locations.
	2. Cloudfront is optimized to work with other Amazon Web Services like S3, EC2, ELB, Route53.Cloudfront also works seamlessly with any non AWS origin server which stores the original definitive versions of files.
	3. Edge locations are not only for reads they can be used for writes as well.
	4. Objects are cached for the life of a TTL.
	5. You can clear cached object but u ll be charged for it.
	6. You can have multiple origins in same distribution.very important.
	7. Default TTL for all objects with Cloud front distribution is 24 hours or 86400 seconds.Minimum TTL is 0.Maximum is 365 days or 31536000 seconds.


* Snowball
	1. Types
		1.1 Standard Snowball - Snowball can import to S3 and export from S3.
		1.2 Snowball Edge - Not just storage capacity but compute capacity as well.Can run lambda functions.Its a portable aws datacenter
		1.3 Snowmobile - Exabyte scale date transfer.100 Petabyte per snow mobile.

* Static website hosting
	1. Endpoint convention - https://<bucketname>.s3-website-<region>.amazonaws.com

Note :- Read S3 FAQs

**************************************************************************************************************************************************

EC2 Service :- 

1. Elastic Compute Cloud.Its the virtual servers in cloud of windows , linux etc
2. EC2 Options 
	2.1 On Demand - Allows u to pay fix rate by hour (or by second) with no commitment.Currently by seconds only for linux instances
	2.2 Reserved - Provides u with the capacity reservation , and offer a significant discount on hourly charges for an instance.1 year or 3 years term
		2.2.1 Standard RI (Reserved Instances) - Upto 75% off on demand
		2.2.2 Convertible RI (Reserve Instances) -  Upto 54% off on demand.Capability to change the attributes of he RIs as long as the exchange results in he creation of reserved instances of equal or greater value.
		2.2.3 Scheduled RI (Reserved Instance) -  These are available to launch within the time windows u reserve.Allows u to match ur capacity reservation to a predictable recurring scheules that only requires a fraction of day , week or month
	2.3 Spot - Enables u to bid whatever price u want for instance capacity,providing for even greater savings if ur applications have flexible start and end times.
	2.4 Dedicated hosts - Physical EC2 server dedicated for ur use.It helps to reduce cost by allowing u to use your existing sever bound software licenses.
	2.5 Usecases for On Demand 
		2.5.1 Users who want low cost and flexibility of EC2
		2.5.2 Application with short term,spiky or unpredictable workloads that cannot be interrupted 
		2.5.3 Applications that are developed or tested on EC2 for the first time.
	2.6 Usecases for Reserved 
		2.6.1 Applications with steady state or predictable usage.
		2.6.2 Applications that require reserved capacity
		2.6.3 Users who are able to make upfront payments to further reduce computing cost
3. Termination protection for EC2 instance is turned off by default,you must turn it on.
4. On an EBS backed instance ,the default action is for the root EBS volume to be deleted when the instance is terminated.
5. EBS root voulmes of your default AMI's cannot be encrypted.You can use third party tools (such as bit locker etc) to encrypt the root volume,or this can be done when creating AMIs in the AWS console or using the API.  		
6. Additional EBS volumes can be encrypted.
7. Commands to mount and unmount EBS volumes
	7.1 Login to the linux ec2 instance using ssh
	7.2 sudo su
	7.3 lsblk
	7.4 mkfs -t ext4 /dev/xvdf
	7.5 mkdir <<directoryname>>	
	7.6 mount /dev/xvdf /durgi
	7.7 lsblk
	7.8 cd /
	7.9 umount /dev/xvdf
8. Steps to upgrade the EBS volume attached to EC2
	8.1 Unmount the volume 
	8.2 Detach the volume
	8.3 Create snapshot from the detached volume
	8.4 Create new volume from the snapshot created.Upgrade the volume type
	8.5 Attach the newly created volume to EC2
	8.6 Mount the volume.
	8.7 Hence the data in the already attached volume is retained and the type of volume is also upgraded 	
9. Meta data URL - it gives u the instance's meta-data and not the user data
		http://169.254.169.254/latest/meta-data/

10. Bash Script
		* #!/bin/bash
		* yum update -y
		* sudo su
		* yum install httpd -y
		* service httpd start
		* chkconfig httpd on
		* cd /var/www/html
		* aws s3 cp s3://durgeshlingarkar-dr/welcome.html index.html

11. EC2 Instances Type
	11.1 D for Density.Speciality is Dense Storage.Use Case is Fileservers/Data Warehousing/Hadoop
	11.2 R for RAM.Speciality is Memory optimized.Use Case is Memory Intensive Apps/DBs 
	11.3 M for Main choice for general purpose apps.Speciality is General Purpose.Usecase is Application Servers
	11.4 C for compute.Speciality is Compute Optimized.CPU intensive Apps/DBs
	11.5 G for graphics.Speciality is graphics intensive.Usecase is Video Encoding/3D Application Streaming
	11.6 I for IOPS.Speciality is High Speed Storage.Usecase is NoSQL DBs,Data Warehousing etc
	11.7 F for FPGA.Speciality is Field Programmable Gate Array.Use case is hardware acceleration for you code.
	11.8 T for cheap general purpose (t2 micro).Speciality is lowest cost general purpose.Use case is webserver/Small DBs
	11.9 P - Graphics (Think pics).Speciality is graphics/general purpose GPU.Use Case is Machine Learning , Bitcoin mining etc.
	11.10 X -  Extreme memory.Speciality is memory optimized.Usecase is SAP HANA/Apache Spark etc


12. AMI's are regional.You can only launch an AMI from the region in which it is stored.However u can copy the AMI's to other regions using the console, command line or the amazon EC2 API. 

13. You cannot move a reserved EC2 instance from one region to another.

14. Reserved instances are available for multi-az deployments.

16. Para-Virtual (PV) & Hardware Virtual Machine (HVM) are the different types of virtualization available on EC2.

17. The underlying Hypervisor for EC2 is "XEN"

18. You are limited to running up to at total of 20 On-Demand instances across the instance family, purchasing 20 Reserved Instances, and requesting 20 Spot Instances per your dynamic Spot limit per region.

19. By default, all accounts are limited to 5 Elastic IP addresses per region.If you need more the 5 Elastic IP addresses, we ask that you apply for your limit to be raised. Any increases will be specific to the region they have been requested for. 

20. Amazon SLA (Service Level Agreement) guarantees a Monthly Uptime Percentage of at least 99.95% for Amazon EC2 and Amazon EBS within a Region.

21. you cannot delete a snapshot of an EBS Volume that is used as the root device of a registered AMI.you have to first deregister the AMI to do so.

22. Using the console, I cannot add a role to an EC2 instance, after that instance has been created and powered up.

Note- Go through EC2 FAQ


EBS Service :- 
	1. Block based storage volumes
	2. It can be attached to EC2 instances
	3. EBS are placed in specific AZ
	4. Automatically replicated to protect from failure.Note that they are not backed up in diff AZ but in same AZ
	5. EBS Types
		5.1 Genral Purpose SSD (GP2)
			5.1.1 Used for balancing price and performace.
			5.1.2 3 IOPS per GB with upto 10000 IOPS.Can burst upto 3000 IOPS
		5.2 Provisioned IOPS SSD
			5.1.2 Used for I/O intensive applications like large relational databases
			5.1.3 Shoud be used if needed more then 10000 IOPS
			5.1.4 Can provision upto 20000 IOPS per volume
		5.3 Throughput Optimized HDD (ST1)
			5.3.1 Used for big data,data warehousing, log procesing
			5.3.2 Throughput Optimized HDD cannot be a boot volume.
		5.4 Cold HDD (SC1)
			5.4.1 Lowest cost storage for infrequent access
			5.4.2 Typical usecase is fileserver
			5.4.3 Cold HDD cannot be boot volumes
		5.5 Magnetic (Standard)
			5.5.1 Lowest cost per giga byte of ALL EBS volume types that is bootable.
			5.5.2 Ideal for workloads where data is access infrequently and lowest storage cost is imp
			5.5.3 More or less same as Cold HDD (SC1).The only diff is that it can be used as a boot volume.
	6. You cannot mount one EBS volumne to multiple EC2 instances.For that u have to use EFS(Elastic File System)
	7. On termination of EC2 instance the associated EBS is deleted by default.We can configure it to not to do so while launching EC2.
	8. EBS volumes can be changed on the fly (except for the magnetic standard)
	9. Best practice is to stop the ec2 instance and then change the volume
	10. If you change the volumne on the fly you must wait for 67 hours before making any changes.
	11. U have to scale EBS volumes up only 
	12. Volumes must be in the same AZ as that of EC2 instances 
	13. To create a snapshot for amazon EBS volumes that serves as root devices , you should stop the instance before taking the snapshot.
	14. Snapshots of encrypted volumes are encrypted automatically.
	15. Volumes restored from encrypted snapshots are encrypted automatically.
	16. You can share snapshots in market place only if they are not encrypted as the encrytion key is tied to ur AWS account.
	17. The unencrypted snapshots can be shared with other AWS accounts or made public.
	18 EBS Volumes vs Instance Store Volumes
		18.1 All AMIs are categorized as either backed by AMAZON EBS or backed by instance store.
		18.2 For EBS volumes ,The root device for an instance launched from the AMI is an Amazon EBS volume created from an amazon EBS snapshot.
		18.3 For Instance Store Volumes,  The root device for an instance launched from the AMI is an instance store volumne created from a template stored in  Amazon S3.
		18.4 Instance store volumes are sometimes called as Ephemeral Storage.
		18.5 Instance store volumes cannot be stopped , if the underying host fails , you ll lose your data.This means instance store backed volumes are not persistent.
		18.6 EBS backed instances can be stopped , You ll not lose the data on this instance if it is stopped.This means EBS backed volumes are persistent.
		18.7 You can reboot both instance store and EBS backed , You ll not lose your data.
		18.8 With instance store ,by default the root volumes ll be deleted on termination.However, with EBS volumes you can tell AWS to keep the root device volume.
		18.9 EBS volumes can be dettached and reattached to other EC2 instances,You cannot do the same with instance store volumes they exist only for the life of that instance. 
	19. Volumes vs Snapshots
		19.1 Volumes exist on EBS (i.e on virtual hard disks)
		19.2 Snaphot exist on S3.
		19.3 You can take snapshots of a volume ,this ll store that volumne on S3.
		19.4 Snapshots are point in time copies of volumes.
		19.5 Snapshots are incremental, this means that only the blocks that have changed since your last snapshot are moved to S3.Hence it takes time to create the first snapshot as against the subsequent snapshots that ll be generated.
		19.6 Snapshots of encrypted volumes are encrypted automatically.
		19.7 Volumes restored from encrypted snapshots are encrypted automatically.
		19.8 You can share snapshots in market place, but only if they are unencrypted.
	20. You can stripe multiple volumes together to achieve up to 75,000 IOPS or 1,750 MiB/s when attached to larger EC2 instances. However, performance for st1 and sc1 scales linearly with volume size so there may not be as much of a benefit to stripe these volumes together.




*************************************************************************************************************************************************

VPC 

1. Amazon Virtual Private Cloud lets u provision a logically isolated section of the Amazon web services cloud where you can launch AWS resources in virtual network that you define.You have complete control over your virtual networking environment, including selection of your IP address range , creation of subnets , and configuration of route tables and network gateways.
2. You can easily customize the network configuration for you Amazon Virtual Private Cloud.For example, you can create a public facing subnet for your webservers that has access to the internet, and place your backend systems such as databases or application servers in a private facing subnet with no internet access.You can also leverage multiple layers of security, including security groups and network access control lists, to help control to Amazon EC2 instances in each subnet.
3. Additionally, you can create a Hardware Virtual Private Network (VPN) connection between your corporate datacenter and your VPC and leverage the AWS cloud as an extension of your corporate datacenter.This setup is also called as hybrid cloud 
4. What can you do with VPC
	4.1 Launch instances into a subnet of your choosing
	4.2 Assign custom IP address ranges for each subnet.
	4.3 Configure route tables between subnets
	4.4 Create an Internet Gateway and attach it to your VPC.
	4.5 Much better security control over your AWS resources.
	4.6 Create security groups for instances
	4.7 Subnet network access control lists(ACLS)
5. Default VPC
	5.1 Default VPC is user friendly , allowing you to immediately deploy instances.
	5.2 All subents in default VPC have a route out to the internet.
	5.3 Each EC2 instance have both a public and private IP address.
	5.4 If u delete the default VPC the only way to get it back is to contact AWS.

6. VPC Peering - VPC peering is simply a connection between 2 VPCs that enables you to route traffic between them using private IP addresses.Instances in either VPC can communicate with each other as if they are within the same network.You can create a VPC peering connection between your own VPCs, or within a VPC in another AWS  account but within a single region.AWS uses the existing infrastructure of a VPC to create a VPC peering connection,It is niether a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware.There is no single point of failure for communication or a bandwidth bottlenecking

	6.1 Allows you to connect one VPC with another via a direct network route using private IP addresses.
	6.2 Instances behave as if they are om the same private network.
	6.3 You can peer VPC's with other AWS accounts as well as with other VPCs in the same account but within a same region.
	6.4 Peering is in a star configuration, i.e 1 central VPC peers with 4 others. No Transitive Peering (i.e one VPC cannot talk to other VPC via a third VPC)
	6.5 VPC Peering Limitations
		6.5.1 You cannot create a VPC peering connection between VPCs that have matching or overlapping CIDR blocks.
		6.5.2 You cannot create a VPC peering connection between VPCs in different regions.
		6.5.3 VPC peering does not support transitive peering relationships.​VPC peering does not support edge to edge routing.​



7. Think of a VPC as logical datacenter in AWS	
8. It consists of IGW's (Internet Gateway or Virtual Private Gateways), Route Tables, Network Access Control Lists, Subnets, Security Groups.
9. One VPC can only have one Internet Gateway.You cannot configure multiple internet gateways to a VPC.
10. One subnet is equal to one AZ.One subnet cannot span across multiple AZs
11. Security Groups are statefull , Network Access Control Lists are stateless
12. There is no Transitive Peering in a VPC.
13. When you create a VPC then Route Tables, Security Groups and Network Access Control Lists are automatically created.
14. NAT Instances
	14.1 When creating a NAT instance, Disable Source/Destination check on instance.
	14.2 NAT instance must be in a public subnet.Once subnet is equal to one AZ.So NAT is always in one AZ.
	14.3 There must be a route out of the private subnet to the NAT instance, in order for this to work.
	14.4 The amount of traffic that a NAT instance supports, depends on the instance size.If you are bottlenecking, increase the instance size.
	14.5 You can create high availability using AutoScaling Groups, Multiple Subnets in different AZs and a script to automate failover.
	14.6 NAT instances are always behind a security group.
15. NAT Gateways
	15.1 NAT gateways are very new.
	15.2 Preffered by the enterprise.
	15.3 Scale automatically up to 10gbps
	15.4 No need to patch.
	15.5 Not associated with security groups.
	15.6 Automatically assigned a public  IP address
	15.7 Remember to update your route table.
	15.8 No need to disable Source/Destination checks.
16. Read NAT Instances vs NAT Gateways in AWS docuentation
17. Security Groups vs Network ACL
	17.1 Security group operates at instance level (first layer of defence) where as Network ACL operates at subnet level (Second Layer of Defence)
	17.2 Security groups supports allow rules only where as Network ACL supports allow rules and deny rules.
	17.3 SGs are stateful (i.e Return traffic is automatically allowed, regardless of any rules) where as Network ACL is stateless (i.e return traffic must be explicitly allowed by rules)	
	17.4 SGs evaluate all rules before deciding whether to allow traffic whereas Network ACL process rules in number order when deciding whether to allow traffic.
	17.5 SGs applies to an instance only if someone specifies the security group when launching the instance, or associate the security group with the instance later on. whereas Network ACL automatically applies to all instances in the subnets it's associated with (backup layer of defence, so you dont have to rely on someone specifying the security group.)
18. Network ACL
	18.1 Your VPC automatically comes with a default Network ACL and by default it allows all outbound and inbound traffic.
	18.2 You can create custom Network ACL.By Default, each custom network ACL denies all inbound and outbound traffic untill you add rules.
	18.3 Each subnet in your VPC must be associated with a network ACL.If you dont explicitly associate a subnet with a Network ACL, the subnet is automatically associated with the default network ACL.
	18.4 You can associate a Network ACL with multiple subnets.However, a subnet can be associated with only one Network ACL at a time.When you associate a network ACL with a subnet, the previous assiciation of the subnet is removed.
	18.5 A network ACL contains a numbered list of rules that is evaluated in order , starting with the lowest numbered rule.
	18.6 A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.
	18.7 Network ACLs are stateless.i.e responses to allowed inbound traffic are subject to the rules for outbound traffic and vice versa.
	18.8 You can block specific IP addresses using network ACLs and not security groups.
19 NAT VS Bastion
	19.1 A NAT is used to provide internet traffic to EC2 instances in private subnets whereas a Bastion is used to securely administer EC2 instances (using SSH or RDP) in private subnets. 
20. Five VPCs are allowed in each AWS region by default.

	



*************************************************************************************************************************************************
Security Groups

1. In security groups as soon as u add inbound rule , the respective outbound rule is added irrespective of explicit configuration of outbound rule.Whatever is allowed in it is allowed out as well.This means security groups stateful means if u add inboud rule it automatically adds outbound rule
2.All inbound traffic is blocked by default
3.All outbound traffic is allowed by default
4. Changes to security groups takes effects immediately
5.You can have any number of EC2 instances within a security group.
6. You can have multiple security groups attached to EC2 instances
7. Security groups are stateful.It means if u create an inbound rule allowing traffic in , the traffic is automatically allowed back again
8. You cannot block specific IP addresses using security groups , instead use Network access control lists
9. You can specify allow rules but cannot deny rules



*************************************************************************************************************************************************
RAID & Snapshot

1. RAID - Stands for Redundant Array of Independent Disk.It means putting bunch of disks together which act as a single disk to the OS
2. Types of RAID
	2.1 RAID 0 - Striped, No Redundancy , Good Performance
	2.2 RAID 1 - Mirrorred, Redundancy
	2.3 RAID 5 - Good for reads ,bad for writes , AWS does not recommend ever putting RAID 5 on EBS
	2.4 RAID 10 - Stripped and Mirrorred , Good Redundancy and Good Performance.
3. Problem in taking snapshot of RAID Array - The snapshot excludes the data held in cache by the applications or the OS.This does not matter wen there is a single volumne.However, in a RAID setup there are multiple volumes with interdependacies which poses a problem
4. How to take a snapshot of a RAID array
	4.1 Stop the application from writing to disk
	4.2 Flush all caches to the disk
5. How to achieve the soutions mentioned in point 4
	5.1 Freeze the file system
	5.2 Unmount the RAID array
	5.3 Stopping the associated EC2 instance  (This is the most easiest of all)	


*************************************************************************************************************************************************
ELB (Elastic Load Balancer) Service - 

1. Instances managed by ELB are reported as InServie or OutOfService
2. Health check checks the instances health by talking to it.
3. ELB have their own DNS name.You are never given an IP address for ELB
4. Read FAQ for classic ELB 


*************************************************************************************************************************************************

Cloud Watch

1. Standard monitoring is 5 mins
2. Detailed monitoring is 1 min
3. With cloud watch u can create dashboards to see what is happening with ur AWS environment
4. Allows u to set alarms that notify you when particular thresholds are hit.Also these alarms are used for auto scaling
5. Cloudwatch events helps u to respond to state changes in AWS resources
6. Cloud watch logs helps u to aggregate , monitor and store logs
7. Cloudwatch is for logging and monitoring resources whereas cloudtrail is for auditing.
8. Amazon CloudWatch stores metrics for terminated Amazon EC2 instances or deleted Elastic Load Balancers for 2 weeks.
9. The Amazon CloudWatch monitoring charge does not vary by Amazon EC2 instance type
10. By default for EC2, cloud watch provides metrics related to CPU,Disk,Network, Status Check (testing both at instance and host level).RAM or memory metrics are not provided by default.To provision memory related metrics you have to create custom metrics.


************************************************************************************************************************************************

Placement group
	1. A placement group is a logical grouping of instances within a single availability zone.Using placement groups enables applications to participate in a low latency ,10 Gbps network.Placement groups are recommended for applications that benefit from low network latency, high network throughput , or both
	2. A placement group cant span across multiple AZs
	3. The name u specify to ur placememt group must be unique within ur AWS account.
	4. Only certain types of instances can be launched in placement group (Compute optimized,GPU,Memory optimized,Storage optimized)
	5. AWS recommend homogenous instances (means instances of same size and same family) within placement group
	6. You cant merge placement groups
	7. You cant move an existing instance in a placement group.You can create an AMI from your existing instance, and then launch a new instance from the AMI into a placement group. 

************************************************************************************************************************************************

EFS (Elastic File System)

1. Amazon Elastic File System is File storage service for Amazon EC2.
2. EFS is easy to use and provides simple interface that allows u to create and configure file systems easily and quickly.
3. With EFS,storage capacity is elastic , growing and shrinking automatically as u add and remove files, so ur applications have the storage they need, when they need it.
4. We cannot mount one EBS volume to 2 or more EC2 instances , thats exactly what EFS allows us to do.
5. EFS supports Network File System version 4 (NFSv4) protocol.
6. You only pay for the storage u use (it means no pre provisioning is required) unlike EBS.
7. EFS can scale up to petabytes
8. EFS can support thousands of concurrent NFS connections.Amazon EFS supports one to thousands of Amazon EC2 instances connecting to a file system concurrently.
9. Data is stored across multiple AZs within a region.
10. EFS is block based storge as opposed to object based storage like the one in S3.We can put files in EFS and share it with other EC2 instances.
11. EFS has read after write consistency just like S3. 	
12. EFS usecase is, it is used as a file server.it acts as a shared repository whose files are accessed  by multiple EC2s.
13. EC2 instances on which the EFS is to be mounted must be in same security group as EFS.

*************************************************************************************************************************************************
LAMDA

1 LAMDA encapsulates 
	1.1 Data centers
	1.2 Hardware
	1.3 Assembly code/Protocols
	1.4 High Level Languages
	1.5 Operating Systems
	1.6 Application Layer/AWS APIs
	1.7 AWS LAMDA

2. AWS LAMDA is a compute service where you can upload your code and create a LAMDA function.AWS LAMDA takes care of provisioning and managing the servers that you use to run the code.You dont have to worry aboubt operating systems,patching , scaling etc.You can  use LAMDA in the following ways.
	2.1 As an event driven compute service where AWS LAMDA runs code in response to the events.These events could be changes to data in an AMAZON S3 bucket or an AMAZON  DynamoDB table.
	2.2 As a compute service to run your code in response to HTTP requests using AMAZON API gateway or API calls made using AWS SDKs.

3. Following are the triggers for LAMDA
	3.1 API Gateway
	3.2 AWS IOT
	3.3 ALEXA Skill kits
	3.4 ALEXA Smart Homes
	3.5 CloudFront
	3.6 Cloudwatch Events
	3.7 Cloudwatch Logs
	3.8 CodeCommit
	3.9 Cognito Sync Trigger
	3.10 DynamoDB
	3.11 Kinesis
	3.12 S3
	3.13 SNS

4. Languages supported by LAMDA
	4.1 Node.js
	4.2 JAVA
	4.3 Python
	4.4 C#

5. LAMDA Pricing
	5.1 LAMDA is priced based on number of request
		5.1.2 Fisrt 1 million request are free. $0.20 per 1 million requests thereafter.
	5.2 LAMDA is priced based on the Duration
		5.2.1 Duration is calculated from the time your code begins executing until it returns or otherwise terminates , rounded up to the nearest 100ms.
		5.2.2 The price depends on the amount of memory allocated to your function.You are charged $0.00001667 for every GB-second used.

6. Why is LAMDA cool
	6.1 No servers
	6.2 Continous scaling
	6.3 Very cheap

7. LAMDA scales out (not scale up) automatically

8. LAMDA functions are independent, 1 event = 1 function

9. LAMDA is serverless

10. LAMDA functions can trigger other LAMDA functions , 1 event can trigger X number of lamda fuctions which in turn can trigger other LAMDA functions.

11. Architectures can get extremely complicated with mutiple LAMDA functions, Debugging can be a nightmare. AWS X-rays allows you to debug what is happening

12. LAMDA can do things globally , You can use it to backup S3 buckets to other S3 buckets etc.

13. LAMDA's duration time is maximum of 5 mins.Post which the execution is terminated or timed out.

*************************************************************************************************************************************************
IAM

1. Roles
	1.1 Role are more secure than storing you access key and secret access key on individual EC2 instances.
	1.2 Roles are easier to manage.
	1.3 Roles can be assigned to an EC2 instance after it has been provisioned using both the command line the AWS console.
	1.4 Roles are universal.You can use them in any region
	




************************************************************************************************************************************************

Route53 - It is a global service and not region specific


1. ELB's do not have predefined IPv4 addresses, you resolve to them using a DNS name.Hence there is a problem suppose u have acloud.guru which is a naked domain name , u ll always need an IPv4 address.To solve this issue amazon created Alias Record.Alias record allows u to resolve to a naked domain name somtimes refered to as zone apex record to a ELBs DNS address.
2. When u make a request through route53 using CNAME you are goin to be charged for that request.But if ur making a request through route53 using Alias records then u wont be charged.Hence given the choice , always choose Alias records over CNAME
3. The last word in the domain name reprents "top level domain" and the second word in the domain name is know as "Second level domain". Eg  in .co.uk , "uk" is top level domain and "co" is second level domain
4. You can get the domain names at following URL. http://www.iana.org/domains/root/db
5. A Records
	- An "A" record is the fundamental type of DNS record and the "A" in A record stands for "Address". The A record is used by computer to translate the name of the domain to the IP address.For eg:- http://www.acloudguru.com might point to http://123.10.10.80
6. TTL
	- The length that a DNS record is cached on either the resolving server or the users own local PC is equal to the value of the "Time To Live" (TTL) in seconds.The lower the time to live, the faster changes to DNS records take to propagate throught the internet.
7. CNAME
	- A Canonical Name (CNAME) can be used to resolve one domain name to another.For example, you may have a mobile website with the domain name http://m.acloudguru.com that is used for when users browse to your domain name on mobile devices.You may want the name http://mobile.acloudguru.com to resolve to this same address.
8. Routing Policy
	8.1 Simple Routing Policy
		8.1.1 This is the default routing policy when u create a new record set.This is most commonly used when you have a single resource that performs a given function for your domain, for example, one web server that serves the content.
		
	8.2 Weighted Routing Policy
		8.2.1 This policy let you split your traffic based on different weights assigned.For eg. You can set 10% of your traffic to go to US-EAST-1 and 90% to go to EU-WEST-1

	8.3 Latency Routing Policy
		8.3.1 Latency based routing allows you to route your traffic based on the lowest network latency for you end user(i.e which region ll give them the fastest response time).
		8.3.2 To use latency based routing you create a latency resource record set for the amazon EC2 (or ELB) resource in each region that hosts your website.When Amazon Route 53 receives a query for your site, it selects the latency resource record set for the region that gives the user the lowest latency. Route53 then responds with the value associated with that resource record set.


	8.4 Failover Routing Policy
		8.4.1 Failover routing policies are used when you want to create and active/passive setup.For example you may want your primary site to be in EU-WEST-2 london region and ur secondary DR site in AP-SOUTHEAST-2 sydney region.
		8.4.2 Route53 ll monitor the health of your primary site using a health check.A heath check monitors the health of your endpoints.
		8.4.3 If the health check fails , then the traffic is routed to your DR site by Route53. 

	8.5 Geolocation Routing Policy
		8.5.1 Geolocation routing lets you choose where your traffic will be sent based on the geographic location of your users(i.e the location from which the DNS queries originate). For Eg:- You might want all queries from Europe to be  routed to a fleet of EC2 instances that are specifically configured for your european customers.These servers may have the local language of your European customers and all prices are displayed in EUROS.

	8.6 Route53 does support MX (Mail Exchange) records 
	
	8.7 There is a limit of 50 domain names u can have using Route53, however this limit can be raised by contacting AWS support.


************************************************************************************************************************************************

RDS Service 

1. Relational Databases under RDS
	1.1.1 SQL server
	1.1.2 Oracle
	1.1.3 MySQL Server
	1.1.4 PostgreSQL
	1.1.5 Aurora
	1.1.6 MariaDB

2. Non Relational Databases/NoSQL Databases under RDS
	1.1 DynamoDB

3. RedShift 
	1. It is for datawarehousing or OLAP service.


4. Elastic Cache
	4.1 Elastic cache is a web service that makes it easy to deploy, operate and scale an in memory cache in cloud.The service improves the performance of web applications by allowing you to retrieve information from fast,managed in memory caches , instead of relying entirely on slower disk based databses.
	4.2 Elastic cache supports 2 open source in memory caching engines 
		4.2.1 Memcached
		4.2.2 Redis
5. DMS (Database Migration service)
	5.1 Allows u to migrate ur production database to AWS.Once the migration has started,AWS manages all the complexities of the migration processes like data type transforation, compression, and parallel transfer(for faster data transfer) while ensuring that data changes to the source database that occur during the migration process are automatically replicated to the target.
	5.2 AWS schema conversion tool automatically converts the source database schema and a majority of the custom code , including views ,stored procedures and functions to a format compatiable with the target database.	 		

6. Datawarehousing vs RDS / OLTP vs OLAP 
	6.1 OLTP differs from OLAP in terms of the typs of queries they run.
	6.2 OLTP eg :- get details of order 1001.Pulls up the row of data name , date , address,delivery status etc.
	6.3 OLAP eg:- Net profit for EMEA and pacific for the digital radio product.Pulls in the large number of records and does operations like summation

7. RDS Backups
	7.1 Automated Backups
		7.1.1 There are 2 diff types of backups for AWS.Automated Backups and Database Snapshots.
		7.1.2 Automated backups allow you to recover your database to any point in time within a "retention period".Retention period can be between 1 and 35 days.Automated backups ll take a full daily snapshot and ll also store transaction logs throught the day.When u do a recovery, AWS ll choose the most recent daily backup, and  then apply transaction logs relevant to that day.This allows you to do a point in time recovery down to a second, within a retention period.
		7.1.3 Automated backups are enabaled by default.This backup data is stored in S3 and you get a free storage space equal to the size of your database.So if you have a RDS instance of 10GB you ll get 10GB worth of storage.
		7.1.4 Backups are taken within a defined window.During the backup window, storage I/O may be suspended while your data is being backed up and you may experience elavated latency.
		7.1.5 In RDS, changes to the backup window take effect immediately	

	7.2 Snapshots
		7.2.1 DB snapshots are done manually (i.e initiated by user).They are stored even after you delete the original RDS instance, unlike automated backups.

	7.3 Whenever you restore either an Automatic Backup or a manual snapshot , the restored version of the database will be a new RDS instance with an new end point.
	
8 Encryption
	8.1 Encryption at rest is supported for MySQL, Oralce, SQL server,PostgreSQL & MariaDB. Encryption is done using AWS Key Management System (KMS) service.Once your RDS instance is encrypted the data stored at rest in the underlying storage is encrypted , as are its automated backups ,read replicas and snapshots.
	8.2 At present time, encrypting an existing DB instance is not supported.To use amazon RDS encryption for an existing database, create a new DB instance with encryption enabled and migrate your data into it.


9 Multi AZ RDS
	9.1 Multi AZ allows u to have an exact copy of your production database in another AZ.AWS handles the replication for you, so when your production database is written to, this write ll automatically be synchronised to the stand by  database.
	9.2 In the event of planned database maintainance, DB instance failure, or an AZ failure, Amanzon RDS ll automatically failover to the standby so that database operations can resume quickly without administrative intervention.
	9.3 Multi AZ is for  disaster recovery only.It is not primarily used for performance improvement.For performance improvement you need read replicas.
	9.4 Multi AZ Databases are applicable for
		9.4.1 SQL Server
		9.4.2 Oracle
		9.4.3 MySQL Server
		9.4.4 PostgreSQL
		9.4.5 MariaDB

10 Read Replica
	10.1 Read replica allows you to have a read only copy of your production database.This is achieved by using asynchronous replication from the primary RDS instance to the read replica.You can use read replica's primarily for very read-heavy database workloads.
	10.2 Read Replicas are supported in 
		10.2.1 MySQL Server
		10.2.2 PostgreSQL
		10.2.3 MariaDB
	10.3 Read replicas are used for scaling and not for DR
	10.4 Must have automatic backups turned on in order to deploy a read replica.
	10.5 You can have upto 5 red replicas copy of any database
	10.6 You can have read replicas of read replicas (but watch out for latency)	
	10.7 Each read replicas is gonna have its own DNS end point.
	10.8 You cannot have Read Replicas that have Multi AZ.
	10.9 You can create read replicas of multi AZ source databases however.
	10.10 Read replicas can be promoted to be their own databases.This breaks the replication.
	10.11 You can also have read replicas in a second region for MySQL and MariaDB.Not for PostgreSQL.

 Note :- Read FAQs of RDS service.

11. DynamoDB vs RDS
	11.1 DynamoDB offers "push button" scaling , meaning that u can scale your database on the fly , without any down time.
	11.2 RDS is not so easy and you usually have to use a bigger instance size or to add a replica.
	11.3 In RDS you can scale up but scale out is only possible in case of reads and not writes.

12. DynamoDB
	12.1 Amazon DynamoDB is fast and flexible NoSQL database service for all applications that need consistent, single digit latency at any scale.It is a fully managed database and supports both document and key value data models.Its flexile data model and reliable performance make it a great fit for mobile,web,gaming ,ad-tech,IOT and many other applications.
	12.2 DynamoDB is always stored on SSD storage.
	12.3 DynamoDB is spread across 3 geographically distinct data centres.
	12.4 DynamoDB has eventual consistent reads by default.
		12.4.1 Consistancy across all copies of data is usually reached within a second.Repeating a read after a short time should return the updated data. (This is a best model for Read Performance)	
	12.5 DynamoDB also supports Strongly Consistent Reads
		12.5.1 A strongly consistent read returns a result that reflect all writes that received a succesful response prior to the read.		
	12.6 Pricing
		12.6.1 Provisioned ThroughPut Capacity
			12.6.1.1 Write throughput $0.0065 per hour for every 10 units.
			12.6.1.2 Read throughput $0.0065 per hour for every 50 units
		12.6.2 Storage cost of $0.25 Per GB per month.
		12.6.3 Pricing Example
				12.6.3.1 Lets assume that ur application needs to perform 1 million writes and 1 million reads per day, while storing 3GB of data.First u need to calculate how many writes and reads per second you need.
				12.6.3.2 One million evenly spread writes per day is equivalent to 1000000(writes)/24(hours)/60(minute)/60(seconds)=11.6 write per second.
				12.6.3.3 A DynamoDB capacity unit can handle 1 write per second, so you need 12 write capacity units.
				12.6.3.4 Similarly, to handle 1 million strongly Consistent reads per day , you need 12 read capacity units.
				12.6.3.5 With Read capacity units, you are billed in blocks of 50.With write capacity units, You are billed in blocks of 10.
				12.6.3.6 To calculate Write capacity units (0.0065/10)*12*24=$1872
				12.3.6.7 To calculate Read Capacity units = (0.0065/50)*12*24=$0.0374
	12.7 DynamoDB allows for the storage of large text and binary objects, but there is a limit of 400 KB.					 							

13. RedShift
	13.1 Amazon RedShift is a fast and powerful, fully managed, Petabyte-scale data warehouse service in the cloud.Customers can start small for just $0.25 per hour with no commitments or upfront costs and scale to a petabyte or more for $1000 per terabyte per year, less than a tenth of most other dataware housing solutions.
	13.2 Redshift is used for OLAP operations.  
	13.3 Redshift Configuration
		13.3.1 Single Node - Upto 160 GB
		13.3.2 Multi Node - If you want to scale more than 160 GB use multi node.
			13.3.2.1 Leader Node - Manages client connections and received queries
			13.3.2.2 Compute Node - Stores data and performs queries and computations.You can have upto 128 compute nodes.
	13.4 Redshift has columnar data storage - Instead of storing data as a series of rows, Amazon Redshift organizes the data by column.Unlike row based system, which are ideal for transaction processing, column based systems are ideal for data warehousing and analytics, where queris often involve aggregates performed over large datasets. Since only the columns involved in the queries are processed and columnar data is stored sequentially on the storage media, column based systems require far fewer I/Os, greatly inproving query performance.
	13.5 Redshift has Advanced Compression - Columnar data stores can be compressed much more than row based data stores because similar data is stored sequentially on disk.Amazon redshift employs multiple compression techniques and can often achieve significant compression relative to traditional relational data stores.In Addition, Amazon Redshift doesnt requires indexes or materialized views and so use less space than traditional relational database systems.When loading data into an empty table, Amazon Redshift automatically samples your data and selects the most appropriate compression scheme.
	13.5 Redshift has Massively Parallel Processing (MPP) - Amazon Redshift automatically distributes data and query across all nodes.Amazon redshift makes it easy to add nodes to your data warehouse and enables you to maintain fast query performance as your data warehouse grows.
	13.6 Redshift Pricing
		13.6.1 Compute node hours - Total number of hours you run across all your compute nodes for the billing period.You are billed for 1 unit per node per hour, so a 3-node data warehouse cluster running persistingly for an entire month would incur 2160 instance hours.You ll not be charged for leader node hour.Only compute nodes ll incur charges.
		13.6.2 Backup - You are charges for BackUp
		13.6.3 Data Transfer - You are charged for data transfer (but only within a VPC not outside it)
	13.7 Redshift Security
		13.7.1 Encrypted in transit using SSL
		13.7.2 Encrypted at rest using AES-256 encryption
		13.7.3 By default Redshift takes care of key management.However you can manage your keys through HSM (Hardware Security Module) or through AWS Key Management Service.
	13.8 Redshift Availability
		13.8.1 Currently only available in 1AZ
		13.8.2 You can restore snapshots of Redshift to new AZs in the event of an outage.
	13.9 Amazon's Redshift uses 1024KB / 1MB block size for its columnar storage.	

14. Elasticache
	14.1 Elasticache is a webservice that makes it easy to deploy, operate and scale an in memory cache in the cloud.The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in memory caches instead of relying entirely on slow disk based databases.
	14.2 Caching improves application performance by storing critical pieces of data in memory for low latency access.Cached information may include the results of I/O intensive database queries or the results of the computationally intensive calculations.
	14.3 Types of ElastiCache
		14.3.1 MemCached
			14.3.1.1 A widely adopted memory object caching system.Elasticache is protocol compliant with Memcached, so popular tools that u use today with existing Memcached environments will work seamlessly with the service.
		14.3.2 Redis
			14.3.2.1 A popular open source in memory key-value store that supports data structures such as sorted set and lists.

	14.4 Elasticache supports Master/Slave replication and multi AZ which can be used to achieve cross AZ redundancy.
	14.5 Scenario - Typically u ll be given a scenario where a particular database is under a lot of stress/load. You ll be asked which service you should use to alleviate this.
		14.4.1 Elasticache is good choice if ur database is particularly read heavy and not prone to frequent changes.
		14.4.2 Redshift is good if the reason your database is feeling stress is because management keep running OLAP transactions on it etc.

15. Aurora
	15.1 Amazon Aurora is a MySQL-Compatiable, relational database engine that combines the speed and availability of high end commercial databases with the simplicity and cost effectiveness of an open source databases.Amazon Aurora provides upto five time better performance than MySQL at a price point one tenth of that of a commercial database while delivering similar performance and availability.
	15.2 Scaling 
		15.2.1 Start with 10GB, scales in 10GB increments to 64TB (Storage AutoScaling)
		15.2.2 Compute resources can scale up to 32vCPUs and 244GB of memory.
	15.3 Two copies of ur data is contained in each availability zone, with minimum of 3 availability zones.6 copies of your data is maintained.
	15.4 Aurora is designed to transparently handle the loss of up to 2 copies of data without affecting database write availability and upto 3 copies without affecting read availability.
	15.5 Aurora storage is also self healing.Data blocks and disks are continously scanned for errors and repaired automatically.

16. By default, the maximum provisioned IOPS capacity on an Oracle and MySQL RDS instance (using provisioned IOPS) is 30,000 IOPS.

17. When replicating data from your primary RDS instance to your secondary RDS instance, the data transfer is free of any charge.

18. When you add a rule to an RDS security group you do  need to specify a port number or protocol

19. If you are using Amazon RDS Provisioned IOPS storage with MySQL and Oracle database engines, the maximum size RDS volume you can have by default is 6TB

20. By default, the maximum provisioned IOPS capacity on an Oracle and MySQL RDS instance (using provisioned IOPS) is 30,000 IOPS.

21. AMAZON RDS does not currently support increasing storage on a SQL Server DB Instance

22. If you want your application to check whether a request generated an error then you look for an "Error" node in the response from the Amazon RDS API

23. In RDS,the maximum size for a Microsoft SQL Server DB instance with SQL server express edition is 10GB.

24. You can "force" a failover for any RDS instance that has Multi AZ configured.Follow this documentation http://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_RebootDBInstance.html

25. MySQL installations default to port number 3306

26. You cannot RDP or SSH in to an RDS instance to see what is going on with the operating system.

27. In RDS, you are not responsible for maintaining OS & Application security patching, antivirus etc.

28. By default, customers are allowed to have up to a total of 40 Amazon RDS DB instances. Of those 40, up to 10 can be Oracle or SQL Server DB instances under the "License Included" model. All 40 can be used for Amazon Aurora, MySQL, MariaDB, PostgreSQL and Oracle or SQL Server under the "BYOL" model. If your application requires more DB instances, you can request additional DB instances 


************************************************************************************************************************************************

SQS (Simple Queue Service)
1. SQS was the first ever AWS service that was publicly available and hence the oldest service. 
2. Amazon SQS is web service that gives you access to a message queue that can be used to store messages while waiting for a computer to process them.Amazon SQS is a distributed queue system that enables web service applications to quickly and reliably queue messages that one component in the application generates to be consumed by another component.A queue is a temporary repository for messages that are waiting for processing.
3. Using SQS you can decouple the components of an application so they run independently, with amazon SQS easing message management between components.Any component of a distributed application can store messages in fail-safe queue.Messages can contain upto 256KB of text in any format.Any component can later retrieve the messages programatically using the SQS API.
4.The queue acts as a buffer between the component producing and saving data, and the component receiving the data for processing.This means the queue resolves the issues that arises if the producer is producing work faster than the consumer can process it, or if the producer or consumer are only intermittenly connected to network.
5. Types of Queue in SQS
	5.1 Standard Queues (default)
		5.1.1 Amazon SQL offers standard as the default queue type.A standard queue lets you have nearly unlimited number of transactions per second.Standard queues gaurantee that the message is delivered atleast once.However, ocassionally(because of the highly distributed architecture that allows high throughput ), more than one copy of the message might be delivered out of order.Standard queue provide best effort ordering which ensures that messges are generally delivered in the same order as they are sent.
	5.2 FIFO Queues
		5.2.1 The FIFO queue complements the standard queue.The most important feature of this queue type are FIFO delivery and exactly once processing: The order in which messages are sent and received is strictly preserved and a message is delivered once and remains available until a consumer processes and deletes it.Duplicates are not introduced into the queue.FIFO queues also support message groups that allow multiple ordered message groups within a single queue.FIFO queues are limited to 300 transactions per second (TPS) , but have all the capabilities of standard queues. 	
	5.3. SQS Keyfacts
		5.3.1 SQS is pull based not push based.
		5.3.2 Messages are 256KB in size.SQS is billed at 64KB Chunks.
		5.3.3 Messages can be kept in the queue from 1 minute to 14 days.The default is 4 days.
		5.3.4 Visibility Time out is the amount of time that the message is invisible in the SQS queue after the reader picks up the message.Provided the job is processed before the visibility time out expires, the message ll then be deleted from the queue.If the job is not processed within that time , the message ll become visible again and another reader ll process it.This could result in the same message being delivered twice.
		5.3.5 Maximum visibility timeout is 12 hours.So if you have jobs/processes that last more than 12 hours, SQS may not be the service u could use.Default visibility time out is 30 seconds.
		5.3.6 SQS gaunrantees that your messages ll be processed atleast once.
		5.3.7 Amzon SQS long polling is a way to retrieve messages from your SQS queues.While the regular short polling returns immediately, even if the message queue being polled is empty, long polling doesnt return a response until a message arrives in the message queue or the long poll times out.Thus long polling is less expensive as compared to short polling as in short polling an EC2 continously polls the queue for messages.Maximum long poll time out is 20 seconds.
	5.4 Your EC2 instances download jobs from the SQS queue, however they are taking too long to process them. What API call can you use to extend the length of time to process the jobs? - Answer is ChangeMessageVisibility API

	5.5 A dead-letter queue is a queue that other (source) queues can target for messages that can't be processed (consumed) successfully.The dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter queue of a standard queue must also be a standard queue.

	5.6 Delay queues let you postpone the delivery of new messages in a queue for the specified number of seconds. If you create a delay queue, any message that you send to that queue is invisible to consumers for the duration of the delay period.

	5.7 For standard queues, the per-queue delay setting isn't retroactive: If you change the DelaySeconds attribute, it doesn't affect the delay of messages already in the queue.

	5.8 For FIFO queues, the per-queue delay setting is retroactive: If you change the DelaySeconds attribute, it affects the delay of messages already in the queue.	

	5.9 The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group (however, messages that belong to different message groups might be processed out of order).

	5.10 When messages that belong to a particular message group ID are invisible, no other consumer can process messages with the same message group ID.

	5.11 



Note - Read FAQs for SQS		

************************************************************************************************************************************************		
SWF Service

1. Amazon Simple Workflow Service (Amazon SWF) is a web service that makes it easy to co ordinate work across distributed application components.Amazon SWF enables applications for a range of use cases, including media processing, web application backends, business process workflows, and analytics pipelines, to be designed as a coordinatin of tasks.Tasks represent invocations of various processing steps in an application which can be performed by executable code, web service calls, human actions and scripts.
2. SWF vs SQS
	2.1 SQS has a retention period of 14 days , SWF upto 1 year for workflow executions.
	2.2 SWF presents a task oriented API , whereas SQS offers a message oriented API
	2.3 SWF ensures that a task is assigned only once and is never duplicated.With SQS you need to handle duplicated messages and may also need to ensure that a message is processed only once.
	2.4 SWF keeps track of all the tasks and events in an application.With SQS you need to implement your own application level tracking, especially if your application uses multiple queues.
3. SWF Actors
	3.1 Workflow Starters - An application that can initiate (start) a workflow.Cloud be your e-commerece website when placing an order or a mobile app searching for bus times.
	3.2 Deciders - Control the flow of activity tasks in a workflow execution.If something has finished in a workflow (or fails) a Decider decides what to do next. Decider is a program that controls the coordination of tasks i.e their ordering, concurrency, and scheduling according to the application logic.
	3.3 Activity Workers - Carry out the activity tasks.Workers are program that interact with Amazon SWF to get the tasks,process received tasks and return the results.
4. Domain refers to a collection of related workflows	
5. The workers and deciders can run on cloud infrastructure such as Amazon EC2 or on machine behind firewalls, Amazon SWF brokers the interactions between workers and the decider.It allows the decider to get consistent views into the progress of tasks and  to initiate new tasks in an ongoing manner.
6. At the same time, Amazon SWf stores tasks, assign the  to workers when they are ready and monitors their progress.It ensures that a task in assigned only once and is never duplicated.Since SWF maintains the application's state durably, workers and deciders dont have to keep track of the execution state.They can run independently and scale quickly.
7. Your workflow and activity types and the workflow execution itself are all scoped to a domain.Domains isolate a set of types, executions  and tasks lists from others within the same account.
8. You can register a domain by using the AWS Management Console or by using the RegisterDomain action in the Amazon SWF API.The parameters are specified in JSON format.
	RegisterDomain
	{
		"name":"9787878"
		,"description":"music"
		,"workflowExecutionRetentionPeriodInDays":"60"
	} 

*************************************************************************************************************************************************

SNS Service

1. Amazon Simple Notification Service (Amazon SNS) is a web service that makes it easy to setup, operate , and send notifications from cloud.It Provides developers with a highly scalable, flexible and cost effective capbility to publish messages from an application and immediately deliver them to the subscribers or other applications.
2. Using SNS , you can push notifications to Apple,Google, Fire OS and Windows Devices as well as Android Devices in china with Baidu Cloud Push.Amazon SNS follows the "publish-subscribe" (pub-sub) message paradigm, with notifications being delivered to clients using a "push" mechanism that eliminates the need to periodically check or "poll" for new information and updates.
3. Besides pushing cloud notifications directly to mobile devices, SNS can also deliver notificatons by SMS text message or email, to Amazon Simple Queue Service (SQS) queues, or to any http endpoint.SNS notifications can also trigger lambda functions.When a message is published to an SNS topic that has a lambda function subscribed to it, the lambda function is invoked with the payload of the published message.The Lambda function receives the message payload as an imput parameter and can manipulate the information in the message, Publish the message to other SNS topics, or send the message to other AWS services.
3. SNS is a push messaging system.
4. SNS allows you to group multiple recipients using topics.A topic is an "access point" for allowing recipients to dynamically subscribe for identical copies of same notification.One topic can support deliveries to multiple endpoint types -- for eg , You can group together IOS,Android and SMS recipients.When u publish once to a topic, SNS delivers appropriately formatted copies of your message to each subsciber.
5. To prevent message from being lost , all messages published to Amazon SNS are stored redundantly across multiple AZs.
6.Benefits of SNS
	6.1 Instantaneos, Push based delivery(no polling)
	6.2 Simple APIs and easy integration with applications.
	6.3 Flexible message delivery over multiple transport protocols.
	6.4 Inexpensive, pay-as-you-go model with no upfront costs.
	6.5 Web-based AWS management console offers the simplicity of a point and click interface.
7. SNS vs SQS
	7.1 Both are messaging services in AWS.
	7.2 SNS is push based.
	7.3 SQS is pull based (Polling)
8. SNS Pricing 
	8.1 Users pay $0.50 per 1 million Amazon SNS requests
	8.2 $0.06 per 100,000 Notification deliveries over HTTP.
	8.3 $0.75 per 100 Notification deliveries over SMS.
	8.4 $2 per 100,000 Notification deliveries over Email.
9. SES stands for Simple Email Service	
10. Following protocols are used in SNS.Messages can be customized for each protocol.
	10.1 HTTP
	10.2 HTTPS
	10.3 Email
	10.4 Email-JSON
	10.5 Amazon SQS
	10.6 Application

************************************************************************************************************************************************

Elastic Transcoder

1. Elastic transcoder is a Media Transcoder in the cloud. 
2. Convert media files from their original source format into different different formats that ll play in smart phones, tablets , PCs etc.It provides transcoding presets for popular output format, which means that u dont need to guess about which setting work best on a particular devices.
3. You can pay based on the minutes that u transcode and the resolution at which you transcode.
4. Read a blog about Elastic transcoder at link "https://read.acloud.guru"

*************************************************************************************************************************************************

API Gateway

1. Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain , monitor and secure APIs at any scale.With the few clicks in AWS management console, You can create an API that acts as a "front door" for applications to access data, business logic, or functionality from your back end services, such as applications running on Amazon Elastic Compute Cloud(EC2), code running on AWS lambda, or any other web application.
2. You can enable API caching in AMAZON API gateway to cache your endpoint's response. With caching, you can reduce the number of calls made to  your endpoint and also improve the latency of requests made to your API.When you enable caching for a stage , API gateway caches respnses from your end point for a specified tine-to-live (TTL) period , in seconds.API gateway then responds to the request by looking up the end point response from the cache instead of making a requst to your endpoint. 
3. API gateways are low on cost efficient.
4. API gateways scales effortlessly.
5. You can throttle requests to prevent attack.
6. API gateways can be connected to cloud watch to log all requests.
7. Same Origin Policy
	7.1 - In computing, the same origin policy is an important concept in the web application security model.Under the policy, a web browser permits scripts contained in the first web page to access data in a second web page, but only if both the webpages have the same origin.
8. Cross-Origin Resource Sharing (CORS)
	8.1 CORS is one way, the server at the other end (not the client code in browser.) can relax the same origin policy.
	8.2 CORS is a mechanism that allows restricted resources (rg fonts) on the webpage to be requested from another domain outside the domain from which the first resource was served.
9. If there is this error "Origin policy cannot be read at the remote resource?".You need to enable CORS on API gateway.

*************************************************************************************************************************************************

Kinesis 

1. Amazon Kinesis is a fully managed service for real time processing of streaming data at massive scale.You can configure hundreds of thousands of data producers to continously put data into Amazon Kinesis streams.For eg data from website click streams, application logs and social media feeds.Within less than a second, the data ll be available for you Amazon Kinesis Applications to read and process from the stream.Amazon Kinesis is a platform on AWS to send your streaming data too.Kinesis makes it easy to load and analyze streaming data, and also providing the ability for you to build your own custom applications for your business needs.

2. What Kinesis Is ?
	2.1 Used to consume big data 	
	2.2 Stream large amount of social media,news feeds logs etc into the cloud


3. Kinesis Core Services
	3.1 Kinesis Streams
		3.1.1 Kinesis streams consist of shards
		3.1.2 Five transactions per second for reads, upto a maximum total data read rate 2MB per second and upto 1000 records per second for writes,  upto maximum total data write rate of 1MB per second (including partition keys)
		3.1.3 The data capacity of your stream is a function of the number of shards that you specify for the stream.The total capacity of the stream is the sum of the capacities of its shards.
		3.1.4 In Knesis streams you can retain data produced by producers for 24 hours by default.But the retention period can be increased upto 7 days. 
	3.2 Kinesis Firehose
		3.2.1 Kinesis Firehose is more automated than Kinesis Strems.You do not have to deal with shards.You can choose to analyze the data produced automatically using lambda and the data is transferred to S3, RedShift etc. 
	3.3 Kinesis Analytics
		3.3.1 Kinesis Analytics enables you to fire SQL like queries on data stored via kinesis.



Note -  Consuming data - think of kinesis (news feeds)
        Business Intelligence - Think of Redshift
        Big Data Processing - Think of Elastic Map Reduce.Elastic Map Reduce allows you root access (i.e u can login using ssh).Big data processing.



************************************************************************************************************************************************
OpsWorks Service
	1. Orchestration service that uses chef 
	2. chef consist of recepies to maintain a consistent state.
	3. Look for the term "chef" or "recepies" or "cook books" and think OpsWork

************************************************************************************************************************************************
AWS Organizations 
	1. It is an account management service that enables u to consolidate multiple AWS accounts into an organization that you create and centrally manage.
	2. It is available in 2 feature set Alhough u can have billing alerts on individual linked accounts as well.
		2.1 Consolidated Billing
			2.1.1 Consolidated billing allows you to get volumne discounts on all your accounts.
			2.1.2 Unused reserved instances for EC2 are applied across the group.
			2.1.3 CloudTrail is on per account and per region basis but can be aggregated into a single bucket in the paying account.
		2.2 All features
	3. At present , by default u can have 20 linked accounts under one paying account.
	4. Billing Alerts - When monitoring is enabled on the paying account the billing data for all linked accounts is included.Alhough u can have billing alerts on individual linked accounts as well.
	5. Cloud trail is Per Account and is enabled per region.You can consolidate logs using an S3 bucket.
		5.1 Turn on cloud trail in the paying account.
		5.2 Create a bucket policy that allows cross account access 
		5.3 Turn on cloud trail in the other accounts and use the bucket in the paying account.


************************************************************************************************************************************************

What are Resource Groups ?
	1. Resource group make it easy to group your resources using the tags that are assigned to them.You can group resources that share one or more tags.
	2. Resources groups contain information such as 
		2.1 Region
		2.2 Name 
		2.3 Health Checks
	3. Specific Information
		3.1 For EC2 - Public and Private IP addresses
		3.2 For ELB - Port Configurations
		3.3 For RDS - Database Engine etc

************************************************************************************************************************************************
Direct Connect
	1. AWS Direct Connect makes ot easy to establish a dedicated network connection from your premises to AWS.Using AWS Direct Connect, You can establish private connectivity between AWS and datacenter, office, or colocation environment which in many cases can reduce your network cost, increase bandwidth throughput and provide a more consistent network experience thousands internet based connections.
	2. Benefits
		2.1 Reduces cost when using large volumes of traffic
		2.2 Increase Reliability
		2.3 Increase Bandwidth
	3. Direct Connects are available in 
		3.1 10gbps
		3.2 1gbps
		3.3 Sub 1gbps can be purchased through AWS Direct Connect Partners
	4. Direct Connect uses VLAN trunking (802.1Q)
	5. VPN vs Direct Connect
		5.1 VPN can  be configured in minutes and are good solutions if u have an immediate need,have low to modest bandwidth requirements, and can tolerate the inherent variability in internet based connectivity.
		5.2 AWS Direct Connect does not involve internet, instead it uses dedicated private network connections between your intranet and Amazon VPC.
	6. It can takes few months to set up a direct connect depending on your requirement	


************************************************************************************************************************************************
Security Token Service (STS)
	1. Grants users limited and temporary access to AWS resources.
	2. Users can come from 3 sources.
		2.1 Federation (typically active directory)
			* Uses security assertion markup language.
			* Grants temporary access based off the users Active Directory Credentials.Does not need to be a user in IAM.
			* Single sign on allows users to log in to AWS console without assigning IAM credentials.
		2.2 Federation with Mobile Apps
			* Uses Facebook/Amazon/Google or other openID providers to log in 
		2.3 Cross Account Access
			* Lets users from one AWS acount access resources in another.
	3. Key Terminology
		3.1 Federation - Combining or joining a list of users in one domain (such as IAM) with a list of users in another domain (such as Active Directory , Facebook etc)
		3.2 Identity Broker - A service that allows you to take an identity from point A and join it (federate it) to point B
		3.3 Identity Store - Services like Active Directory, Facebook, Google
		3.4 Identities - A user of service like Facebook etc.
	4. Scenario -  You are hosting a company website on some EC2 web servers in your VPC.Users of this website must login to the site which then authenticates against the company's active directory servers which are based on site at the companies headquaters.Your VPC is connected to your company HQ via a secure IPSEC VPN.Once logged in the users can only have access to their own S3 buckets.How do you set up  this ?
		4.1 Employees encode the username and password into the portal.
		4.2 These credentials are then forwarded to Identity Broker.
		4.3  Identity Broker checks for the password credentials in company's LDAP server
		4.4 If the credentials are valid identity broker makes a call to AWS STS 
		4.5 STS authenticates and returns temporary security credentials.It contains following 4 things
			* Access Key
			* Secret Access Key
			* Token
			* Duration for which the token is valid.This duration can be between 1 and 36 hours.
		4.6 Once the Identity Broker has that it returns the same to the Portal Application
		4.7 Portal Application makes a call to S3 passing the credentials 
		4.8 S3 uses IAM to verify credentials passed by portal application.If credentials are valid, it allows the requested operation. 



************************************************************************************************************************************************
Workspaces
	1. It's basically a VDI.A workspace is a cloud based replacement for a traditional desktop.A workspace is available as a bundle of compute resources, storage space, and software application access that allow a user to perform day-to-day tasks just like using a traditional desktop.A user can connect to a workspace from any supported device (PC,MAC,Chromebook,iPAD,Andrroid Tablets) using free Amazon workspaces client application and credentials set up by an administrator or  their exsting active directory credentials if Amazon Workspaces is integrated with an existing Active Directory Domain.
	2. Windows 7 experience, provided by windows server 2008 R2.
	3. By default, users can personalize their workspaces with their favorite settings for items such as wallpaper, icons, shortcuts etc.This can be locked down by an administrator however.
	4. By default u ll be given a local administrator access,you can install your own applications.
	5. Workspaces are persistent.
	6. All data on D:/ is backed up every 12 hours.
	7. You do not need an AWS account to login to workspaces.	  


************************************************************************************************************************************************

Elastic Container Service (ECS)

ECS is Amazon's Docker Management Service.Allows you to manage docker containers on a cluster of EC2s

	1. What is Docker 
		* Docker is a software platform that allows u to build, test and deploy applications quickly.
		* Docker is highly reliable.You can quickly deploy and scale applications into any environment and know your code ll run.
		* Docker is infinitely scalable.Running docker on AWS is a great way to run distributed applications at any scale.
		* Docker packages software into standardized units called containers.Containers allows u to easily package an application's code, configuration and dependencies into easy to use building blocks that deliver environmental consistency, operation efficiency, developer productivity and version control.

	2. Containarization Benefits
		* Escape from dependancy hell
		* Consistent progression from DEV -> Test -> QA -> Prod
		* Isolation - performance and stability in App A in Container A , wont affect App B in container B.
		* Much better resource management
		* Extreme code portability
		* Micro services

	3. Docker Coponents
		* Docker Image
		* Docker Container
		* Layer/Union File System
		* Docker File
		* Docker Daemon/Engine
		* Docker Client
		* Docker Registries/Docker Hub	  


ECS
	1. ECS is highly scalable , fast , container management service that makes it easy to run , stop and manage docker containers on a cluster of EC2 instances.ECS lets u launch and stop container based applications with simple API calls, allows you to get the state of ur cluster from a centralized service, and gives you access to many familier EC2 features.ECS is Amazon's managed version of docker.

	2. ECS is a regional service that u can use in one or more AZs across a new or existing VPC to schedule the placement of containers across your cluster based on your resources needs, isolation policies and availability requirements.

	3. AMAZON ECS eliminatates the need for you to operate your own cluster management and configuration management systems, or to worry about scaling your management infrastructure.

	4. ECS can also be used to create a consistent deployment and build experience, manage and scale batch and ETL(Extract,Transform and Load) workloads, and build sophisticated application architectures on a microservices model.

	5. Containers
		5.1 Containers are a method of operating system virtualization that allow u to run an application and its dependencies in resource-isolated processes.
		5.2 Containers have everthing that the software needs to run - including libraries , system tools , code and runtime.
		5.3 Containers are created from a read only template called an image.


	6. ECS Task Definition
		6.1 A Task Definition is required to run docker containers in Amazon ECS.Task Definitions are text files in JSON format that describe one or more containers that form your application.
		6.2 Some of the parameters you can specify in a task definition includes.
			* Which docker images to use with the containers in your task.
			* How much CPU and memory to use with each container
			* Whether containers are linked together in task.

	7. ECS Clusters
		7.1 ECS cluster is a logical gouping of container instances that u can place tasks on.When u first use the ECS service , a default cluster is created for u,but u can create multiple clusters in an account to keep your resource separate.
		7.2 Concept 
			* Clusters can contain multiple different container instance type
			* Clusters are region specific
			* Container instances can be a part of one cluster at a time.
			* You can create IAM policies for your cluster to allow or restrict user access to specific cluster

	8. 		
Note - Read FAQs

************************************************************************************************************************************************
Overview of Amazon AWS - White Paper
	
1. What is Cloud Computing - Cloud computing is the on demand delivery of IT resources and applications via internet with pay as you go pricing.Cloud computing provides a simple way to access servers, storage, databases and a broad set of application services over the internet.Cloud computing providers such as AWS own and maintain the network-connected hardware required for these application services, while you provision and use what you need using a web application.

2. Advantages of cloud
	2.1 Trade capital expenses for variable expenses.
	2.2 Benefit from massive economies of scale.
	2.3 Stop guessing about capacity
	2.4 Increase speed and agility
	2.5 Stop spending money running and maintaining datacenters.
	2.6 Go global in minutes

************************************************************************************************************************************************



************************************************************************************************************************************************

Overview of security processes - White Paper

1. Shared Security Model- AWS is responsible for securing the underlying infrastructure that supports  the cloud, and you are responsible for anything that you put  on the cloud or connect to the cloud. 

2. Provides protection against
	2.1 DDoS
	2.2 Man in the middle attacks
	2.3 IP Spoofing
	2.4 Port Scanning
	2.5 Packet sniffing by other tenants

3. Unauthorized port scans by Amazon EC2 customers are violation of AWS Acceptable Use Policy.You may request permission to conduct vulnerability scans as required to meet your specific compliance requirements.These scans must be limited to your own instances and must not violate the AWS Accptable Use Policy.You must request a  vulnerability scan in adcance.

4. AWS Trusted Advisor Service- Inspects your AWS environment and makes recommendations when opportunities may exist to save money, improve system performace, or close security gaps.It provides alerts on several of the most common security misconfigurations that can occur, including leaving certain ports operating that make you vulnerable to hacking and unauthorized access, neglecting to create IMA accounts for  your internal users, Allowing public access to amazon s3 buckets, not turning on user activity loggin (AWS CloudTrail) and not using MFA on your root AWS Account.


5. Instance Isolation - 
	5.1 Different instances running on the same physical machine are isolated from each other via the Xen Hypervisor.In addition, the AWS firewall resides within the hypervisor layer, between the physical network interface and instance's virtual interface.
	5.2 All packets must pass through this layer, thus an instance's neighbors have no more access to that instance than any other host on the internet can be treated as if they are on separate physical hosts.The Physical RAM is separated using similar mechanisms.
	5.3 Customer instances have no access to raw disk devices, but instead are presented with virtual disks.The AWS proprietary disk virtualization layer automatically resets every block of storage used by the customer, so that one's customer data is never unintentionally exposed to another.
	5.4 In addition, memory allocated to guests is scrubbed (set to zero) by the hypervisor when it is unallocated to a guest.The memory is not returned to the pool of free memory available for new allocations until the  memory scurbbing is complete.


6. Other Considerations
	6.1 Guest Operating System - Virtual instances are completely controlled by you, the customer.You have full root access or administrative control over accounts, services, and applications.AWS does not have any access rights to your instances or the guest OS.Ecryption of sensitive data is generally a good security practice, and AWS provides the ability to encrypt the EBS volumes and their snap shots with AES-256.The encryption occurrs on the servers that host the EC2 instances, providing encryption of data as it moves between EC2 instances and EBS storage.In order to be able to this efficiently and with low latency, the EBS encryption feature is only available on EC2s more powerful instance types (eg M3,C3,R3,G2)

	6.2 Elastic Load Balancing - SSL termination on load balancer is supported.Allows u to identify the originating IP address of a client connecting to your servers, whether you are using https or TCP load balancing.

	6.3 Firewall - Amazon EC2 provides a complete firewall solution.This mandatory inbound firewall is configured in a default deny-all mode and Amazon EC2 customers must explicitly open the ports  needed to allow inbound traffic.

************************************************************************************************************************************************


Storage Options in Cloud - White Paper

1. Storage Gateway
	1.1 AWS Storage Gateway's software appliance is available for download as a virtual machine (VM) image that you install on a host in your datacenter.Once you have installed your gateway and associated it with your AWS account through our activation process, you can use the AWS management console to create either 	
		* gateway-cached
		* gateway-stored
	that can be mounted as iSCSI devices by your on premises application.
	1.2 Gateway-Cached - It allows you to utilize amazon s3 for your primary data, while retaining some portion of it locally in a cache for frequently accessed data.These volumnes minimize the need to scale your on premises storage infrastructure, while still providing your applications with low latency access to their frequently accessed data.You can create storage volumes upto 32TBs in size and mount them as iSCSI devices from your on premises application servers.Data written to these volumes is stored in S3, with only the cache of recently written and recently read data stored locally on your on premises storage hardware.
	1.3 Gateway-Stored - It stores your primary data locally, while asynchronously backing up the data to AWS.These volumes provide your on premises applications with low latency access to their entire datasets, while providing durable, off site backups.You can create storage volumes upto 1 TB size and  mount them as iSCSI devices from your on premises application servers.Data written to your gateway-stored volumes is stored on your on premises storage hardware, and asynchronously backed up to amazon S3 in the form of AMAZON EBS snapshots.  
	1.4 Pricing - With AWS Storgae Gateway , you only pay for what you use.AWS storage gateway has four pricing components:gateway usage(per gateway per month), snapshot storage usage(per GB per month), volumne storage usage(per GB per month), and data transfer out (per GB per month).
	1.5 Gateway-Accessed volumes is not a valid configuration type for AWS storage gateways

*************************************************************************************************************************************************

Architechting For AWS Cloud - Best Practices - White Paper

Benefits of cloud
	1. Almost 0 upfront infrastructure investment
	2. just-in-time infrastructure
	3. More efficient resource utilization
	4. Usage-based costing
	5. Reduced time to market

Technical Benefits
	1. Automation - Scriptable Infrastructure
	2. Auto-Scaling
	3. Proactive Scaling
	4. More efficient development cycle
	5. Improved testability
	6. Disaster recovery and Business continuity
	7. "Overflow" the traffic to the cloud

Design For Failure
	1. Rule of Thumb: Be a pessimist  when designing architectures in the cloud.Assume that things ll fail.In the other words, always design, implement and deploy for automated recovery from failure.In Particular, assume that your hardware will fail.Assume that outages ll occur.Assume that some disaster ll strike your application.Assume that you ll be slammed with more than the expected number of requests per second some day.Assume that with time your application software ll fail too.By being a pesimist, you end up thinking aboubt recovery strategies during design time, which helps in designing an overall system better.

Decouple Your Components
	1. The key is to build components that do not have tight dependencies on each other, so that if one component were to die (fail), sleep (not respond) or remain busy (slow to respond) for some reason, the other component in the system are built so as to continue to work as if no failure is happening.
	2. In essence, loose coupling isolates the various layers and components of your application so that each component intracts asynchronously with the others and treats them as a "black box".

Implement Elasticity
	1. Elasticity can be implementd in 3 ways
		1. Proactive cyclic scaling - Periodic scaling that occurs at fixed interval (daily,weekly,monthly,quaterly)
		2. Proactive Event Based Scaling - Scaling just wen you are expecting a big surge of traffic requests due to a scheduled business event (new product launch, marketing campaingns)
		3. Auto Scaling based on demand - By using a monitoring service, your system can send triggers to take appropriate actions so that it scales up or down based on metrics (utilization of servers or network i/o , for instance)		  	

*************************************************************************************************************************************************
The Well Architected Framework - White Paper

- What is a Well Architected Framework
	1. The well architected framework is a set of questions that you can use to evaluate how well your architecture is aligned to AWS best practices.

- 5 Pillars of the well architected framework.
	1. Security
		1.1 Design Principle
			1.1.1 Apply security at all layers.
			1.1.2 Enable tracebility
			1.1.3 Automate responses to security events
			1.1.4 Focus on securing your system
			1.1.5 Automate security best practices
			1.1.6 
		1.2 Definition
			1.2.1 Security in cloud consist of 4 areas
				1.2.1.1 Data Protection - 
						* Before you begin to architect security practices across your environment, basic data clasification should  be in place.You should organize and classiy your data into segments such as publicly available, available to only members of your organisation, available to only certain members of your organisation, available only to the board etc.You should also implement a least privilege access system so that people are only able to access what they need.However, most importantly you should encrypt everything whereever possible, whether it is at rest or in transit.
						* Data Protection Questions
							- How are you encrypting anf protecting your data at rest ?
							- How are you encrypting anf protecting your data in transit ? (SSL)
				1.2.1.2 Privilege Management
						* Privilenge meanagement ensures that only authorized and authenticated users are able to access your resources, and only in a manner that is intended.It can include
							- Access Control Lists (ACLs)
							- Role Based Access Controls
							- Password Management  (Such as password rotation policy)
						* Privilege Management Questions
							- How are you protecting access to and use of the AWS root account credentials ?
							- How are you defining roles and responsibilites of system users to control human access to the AWS management console and APIs ?	
							- How are limiting automated access (such as applications, scripts, or third party tools or services) to AWS resources ?
							- How are you managing keys and credentials ?
				1.2.1.3 Infrastructure protection
						* Outside of cloud, this is how you protect your datacenter.RFID controls,security,locable cabinets, cctv etc.Within AWS they handle this.So,  really infrastructure exists at a VPC level.
						* Infrastructure protection Questions 
							- How are you enforcing network and host-level boundary protection ?
							- How are enforcing AWS service level protection ?
							- How are you protecting the integrity of the operating systems on your Amazon EC2 instances ?
				1.2.1.4 Detective Controls	
						* You can use detective controls to detect or identify a security breach.AWS services to achieve this includes 
							- AWS cloud trail 
							- AWS cloud watch
							- AWS config
							- AWS S3
							- AWS Glacier
						* Detective Controls Questions
							- How are you capturing and analyzing AWS logs ?	
					
 
	2. Reliability
		2.1 The reliability pillar covers the ability of the system to recover from service or infrastrucutre outgaes/disruptions as well as the ability to dynamically acquire computing resources to meet demand.
		2.2 Design Principles
			* Test Recovery Procedures
			* Automatically recover from failures
			* Scale horizontally to increase aggregate system availability
			* Stop guessing capacity
		2.3 Definition
			* Reliability in cloud consists of 3 areas
				- Foundations
					- In traditional IT, one of the first things you should consider is the size of the communication link between ur headquaters and ur datacenter.If u misprovision this link, it can take 3-6 months to upgrade which can cause a huge disruption to your traditional IT estate.
					- With AWS, they handle most of the foundations for you.The cloud is designed to be esentially limitless meaning the AWS handles the networking and compute requirements themselves.However, they do set service limits to stop customers from accidently over-provisioning resources.
				- Foundations Questions
					- How are you managing AWS service limits for your account ?
					- How you are planning your network topology on AWS ?
					- Do you have an escalation path to deal with technical issues ?
				- Change Management
					- You need to be aware of how change affects system so that u can plan proactively around it.Monitoring allows u to detect any any changes to your environment and react.In traditional systems, change control is done manually and are carefully co-ordinated with auditing.With AWS things are lot easier, you can use cloud watch to monitor your environment and services such as auto scaling to automate change in response to changes on your production environment.
				- Change Management Questions 
					- how does your system adapt to changes in demand ?
					- How are you monitoring AWS resources ?
					- How are you executing change management ?
				- Failure Management	
					- With cloud, you should always architect your systems with the assumptions that failure will occur.You should become aware of these failures, how they occurred and how to respond to them and then plan on how to prevent these from happening again.
				- Failure Management Questions
					- How are you backing up your data ?
					- How does your system withstand component failures ?
					- How are you planning for recovery ?

	3. Performance Efficiency
		3.1 The performance efficiency pillar focus on how to use computing resources efficiently to meet your requirements and  how to maintain that efficeincy as demand changes and technology evoles.
		3.2 Design Principles
			* Democratize advanced technologies.
			* Go global in minutes
			* Use serverless architechtures
			* Experiment more often
		3.3 Definition
			* Compute 
				- When architecting with your system it is important to choose right kind of server.Some apps require heavy CPU utilization, some require heavy memory utilization etc.
				- With AWS servers are virtualized with a click of a button (API calls).You can change the type of server in which your environmnt is running on.You can even switch to running with no servers at all and use AWS lambda.
			* Compute Questions
				- How do you select the appropriate instance type for your system ?
				- How do you ensure that you continue to have the most appropriate instance type as a new instance types and features are introduced ?
				- How do you monitor youu instance post launch to ensure that they are performing as expected ?
				- How do you ensure that the quantity of your instances matches demand ?
			* Storage
				- The optimal storage soltions for your environment depends on number of factors.
					* Access Method - Block, File or Object
					* Patters of Access - Ramdom or Sequential
					* Throughput Required
					* Frequency of Access - Online,Offline or Archival
					* Frequency of Update - Worm, Dynamic
					* Availability Contraints
					* Durability Constraints
				- Storage Questions 
					* How do you select the appropriate storage solution for your system ?
					* How do you ensure that you continue to have the most appropriate storage solution as new storage solutions and features are launched ?
					* How do you monitor your storage solution to ensure that it is performing as expected ?
					* How do you ensure that the capacity and throughput of your storage solutions matches demand ?

			* Database
				- The optimal solution depends on a number of factors.Do you need database consistency, do u need high availability, do you need No-SQL, Do you need DR etc ?
				- With AWS you get a LOT of options.RDS, DynamoDB, Redshift etc.
			* Database Questions 
				- How do you select the appropriate database solution for your system ?
				- How do you ensure that you continue to have the most apropriate database solution and features as new database solution and features are launched ?
				- How do you monitor your databases to ensure performance is as expected ?
				- How do you ensure the capacity and throughput of ur databases matches demand ?	
			* Space-time tradeoff	 
				- Questions 
					* How do u select the appropriate proximity and caching solutions for your system ?
					* How do you ensure that u continue to have the most appropriate proximity and caching solutions as new solutions are launched ?
					* How do u monitor ur proximity and caching solutions to ensure performance is as expected ?
					* How do u ensure that the proximity and caching solutions you have matches demand ?

	4. Cost Optimization
		4.1 Use cost optimization pillar to reduce your cost to an minimum and use those services for other parts of your business.A cost optimized system allows u to pay the lowest price possible while still achieving your business objectives.
		4.2 Design Principles
			* Transparently attribute expenditure
			* Use managed services to reduce cost of ownership
			* Trade capital expense for operating expense.
			* Benefit from economies of scale.
			* Stop spending money on data center operations.
		4.3 Definition
			* Cost optimization in cloud consist of 4 areas
				- Matched supply and demand
					* Try to optimally align supply with demand.Dont over provision or under provision, instead as demand grows , so should the supply of your compute resources.Like Autoscaling which scale with demand.Similarly , in a serverless context, use services such as  Lambda that only execute when a request comes in.Services such as cloud watch can also help you keep track as to what your demand is.
				- Matched supply and demand questions 
					* How do you make sure your capacity matches but does not substantially exceed what u need ?
				- Cost-Effective Resources
					* Using a correct instance type can be a key to cost savings.For eg you might have a reporting process that is running on a t2.micro and it takes 7 hours to complete.The same process can be run on  an  m4.2xlarge in few minutes.The result is same but t2.micro is expensive because it ran fr longer.A well archoitected system will use most cost effiient resources to reach the eand business goal.
				- Cost-Effective Resources Questions
					* Have you selected the appropriate resource types to meet your costs targets ?
					* Have u selected the appropriate pricing model to meet ur cost targets ?
					* Are there managed services (EC2,EBS,S3) that u can use to improve ROI ?
				- Expenditure Awareness
					* With cloud u no longer have to go out and get quotes on physical servers,choose a supplier,have those resources delivered, installed and made available etc.You can provision things within seconds, however it comes with its own issues.Many organizations have different teams each with their own  AWS accounts.Being aware of what each team is spending and where is very crucial to any well architected  system.You can use cost allocation tags to track this, Billing alerts as well as consolidated billing.
				- Expenditure Awareness Questions
					* What access controls and procedures do u have in place to govern AWS cost ?
					* How are you monitoring usage and spending ?
					* How do u decommision resources that u no longer need, or stop resources that are temporarily not needed ?
					* How do u consider data transfer charges when designing your architecture ? 	

				- Optimizing Overtime

	5. Operational Excellence

- Structure of Each Pillar
	1. Design Principles
	2. Definition
	3. Best Practices
	4. Key AWS Services
	5. Resources

- General Design Principles
	1. Stop guessing your capacity needs.
	2. Test Systems at Production Scale
	3. Automate to make acrhitectural experimentation easier
	4. Allow for evolutionary architectures
	5. Data driven architechtures
	6. Improve through game days

 	

The four levels of AWS premium support
	1. Basic 
	2. Developer
	3. Business - The maximum response time for a Business Level Premium Support Case is 1 hour
	4. Enterprise

 

The AWS platform is certified PCI DSS 1.0 compliant

The AWS platform consists of 14 regions currently	

The AWS platform does not provide you much protection against social engineering attacks the rest of the attacks it does provide you protection against.

The names of the AZs are randomly applied, so 'eu-west-1b' is not necessarily the same physical location for all the accounts you own.Availability Zone names are unique per account and do not represent a specific set of physical resources.


Amazon Snowball has replaced the older AWS Import/Export Disk service.

AutoScaling scales-in according to a hierarchy of decisions. Please see the link for further details: http://amzn.to/2lSm9k6

Question - A client who is using EC2 believes that someone other than approved administrators is trying to gain access to her Linux web app instances, and she asks what sort of network access logging can be added to the system. Which of the following might you recommend?
Ans - You should make use of an OS level logging tools such as iptables and log events to CloudWatch or S3.




Route53 has a security feature that prevents internal DNS from being read by external sources. The work around is to create a EC2 hosted DNS instance that does zone transfers from the internal DNS, and allows itself to be queried by external servers.

The public address is not managed on the instance, it is instead an alias applied as a network address translation of the Private IP address.

What is pre warming of Elastic Load Balancer ? Check for this question -- Refer link https://petrutandrei.wordpress.com/2016/03/18/pre-warming-the-load-balancer-in-aws/

AWS does not copy launch permissions, user-defined tags, or Amazon S3 bucket permissions from the source AMI to the new AMI.


AWS offers 4 different levels of support - Basic, Developer,Business,Enterprise. Refer AWS documentation -- Refer link https://aws.amazon.com/premiumsupport/compare-plans/

checkout link - http://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_RebootDBInstance.html

read more about auto scaling, cloud formation, EMR, resource groups, ECS

Power User Access allows access to all aws services except for management of groups and users within IAM.